{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "899a7258-06c1-4781-86d0-d69ce8c834ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORT LIBS AND DEFINE FUNCTIONS ####\n",
    "import os, sys, subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## FUNCTIONS FOR CAS-OFFINDER\n",
    "def generate_casoff_input_file(ref_genome_fa_path, protein, pam, \n",
    "                               n_mismatch, grna_list, len_spacer = 23, n_bulges = 1):\n",
    "    pam_alias = \"N\" * len(pam)\n",
    "    guide_bulge_pattern = f\"{'N' * len_spacer + pam} {n_bulges} {n_bulges}\"\n",
    "    #guide_bulge_pattern = f\"{'N' * len_spacer + pam}\"\n",
    "    input_file_content = [ref_genome_fa_path, guide_bulge_pattern] + [f\"{x}{pam_alias} {n_mismatch}\" for x in grna_list]\n",
    "    input_file_name = f\"{protein}_{pam}_Input_File_v3.txt\"\n",
    "\n",
    "    with open(input_file_name, \"w\") as f:\n",
    "        f.writelines(\"\\n\".join(input_file_content))\n",
    "\n",
    "def generate_validation_str(row, trim_len):\n",
    "    \"\"\"\n",
    "    - Creates string-representations for simpler interpretation of genomic hits\n",
    "    - Let:\n",
    "            - 0 = Mismatch\n",
    "            - D = DNA_bulge\n",
    "            - R = RNA_bulge\n",
    "    \"\"\"\n",
    "    out_str = []\n",
    "    crRNA_str = row[\"crRNA\"][:-trim_len]\n",
    "    dna_str = row[\"DNA\"][:-trim_len]\n",
    "    \n",
    "    for i in range(len(dna_str)):\n",
    "        a = crRNA_str[i]\n",
    "        b = dna_str[i]\n",
    "\n",
    "        if a == b:\n",
    "            out_str.append(\"1\")\n",
    "        elif a == '-':\n",
    "            out_str.append(\"R\")\n",
    "        elif b == '-':\n",
    "            out_str.append(\"D\")\n",
    "        elif b.islower():\n",
    "            out_str.append(\"0\")\n",
    "    return \"\".join(out_str)\n",
    "\n",
    "def count_mismatches(s):\n",
    "    total_count = s.count('0') + s.count(\"R\") + s.count('D')\n",
    "    return total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361fe5eb-259b-4739-b98f-fb890e7f74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SETUP CAS-OFFINDER ####\n",
    "pwd = \"/home/ubuntu/software/cas_offinder_v3/\"\n",
    "conda_env_name = \"be_off\"\n",
    "\n",
    "## PARSE/FORMAT gRNA LIBRARY AND DETAILS\n",
    "ref_genome_fa_path = \"/home/ubuntu/software/cas_offinder_v3/GRCh38.p14.genome.fa\"\n",
    "protein_pam_dict = {\"wt_geo_paper_pam\" : \"NNNNCWAA\",\n",
    "                    \"igeo\" : \"NNNNCNNN\",\n",
    "                    \"wt_nme2cas9\" : \"NNNNCC\",\n",
    "                    \"inme_prelim1\":\"NNNNCH\"\n",
    "                   }\n",
    "\n",
    "# MAKE INPUT FILES\n",
    "grnas = [\"AGCATGATACATGGAATGAGGGA\", \n",
    "         \"ACTGCATAAGGGAGGAAGAACGC\", \n",
    "         \"CTCCTTCCTAGTCTCCTGATATT\", \n",
    "         \"ATATCAGGAGACTAGGAAGGAGG\"]\n",
    "for protein,pam in protein_pam_dict.items():\n",
    "    generate_casoff_input_file(ref_genome_fa_path, protein, pam = pam, n_mismatch = 5, grna_list = grnas)\n",
    "\n",
    "\n",
    "## WRITE BASH SCRIPT\n",
    "job_name = \"cas_offinder_iGEO\"\n",
    "script_name = f\"CasOffinder_iGEO.sh\"\n",
    "cas_off_path = \"/home/ubuntu/software/cas_offinder_v3/cas-offinder-3.0.0b3/cas-offinder\"\n",
    "bash_lines = [\"#!/bin/bash\",\"#SBATCH -p gpu\", f\"#SBATCH --job-name {job_name}\", f\"#SBATCH -o %j.out\",\"#SBATCH -e %j.err\"]\n",
    "\n",
    "cas_offs = []\n",
    "output_files = []\n",
    "for protein,pam in protein_pam_dict.items():\n",
    "    input_file = f\"{protein}_{pam}_Input_File_v3.txt\"\n",
    "    output_file = f\"{pwd}{protein}_{pam}_Output_File_v3.txt\"\n",
    "    output_files.append(output_file)\n",
    "    cas_off = f\"{cas_off_path} {pwd}{input_file} G {output_file}\"\n",
    "    cas_offs.append(cas_off)\n",
    "\n",
    "with open(script_name, \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(bash_lines+cas_offs))\n",
    "\n",
    "# RUN CAS-OFFINDER    \n",
    "#os.system(f\"sbatch {script_name}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253916c2-4f0c-4b7e-abfc-58881d655063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### PROCESSING: wt_geo_paper_pam_NNNNCWAA, wt_geo_paper_pam, NNNNCWAA\n",
      "# N Unfiltered: 5580\n",
      "# TOTAL OFF-TARGETS (Passing < 2 Seed-Region Mismatches & Bulges): 4411\n",
      "#### PROCESSING: igeo_NNNNCNNN, igeo, NNNNCNNN\n",
      "# N Unfiltered: 158947\n",
      "# TOTAL OFF-TARGETS (Passing < 2 Seed-Region Mismatches & Bulges): 134077\n",
      "#### PROCESSING: wt_nme2cas9_NNNNCC, wt_nme2cas9, NNNNCC\n",
      "# N Unfiltered: 33259\n",
      "# TOTAL OFF-TARGETS (Passing < 2 Seed-Region Mismatches & Bulges): 27176\n",
      "#### PROCESSING: inme_prelim1_NNNNCH, inme_prelim1, NNNNCH\n",
      "# N Unfiltered: 152356\n",
      "# TOTAL OFF-TARGETS (Passing < 2 Seed-Region Mismatches & Bulges): 128428\n",
      "#### PROCESSING: wt_geo_paper_pam_NNNNCWAA\n",
      "!!!!Skipping wt_geo_paper_pam_NNNNCWAA\n",
      "# Added igeo_NNNNCNNN\n",
      "# Added wt_nme2cas9_NNNNCC\n",
      "# Added inme_prelim1_NNNNCH\n",
      "#### PROCESSING: igeo_NNNNCNNN\n",
      "# Added wt_geo_paper_pam_NNNNCWAA\n",
      "!!!!Skipping igeo_NNNNCNNN\n",
      "# Added wt_nme2cas9_NNNNCC\n",
      "# Added inme_prelim1_NNNNCH\n",
      "#### PROCESSING: wt_nme2cas9_NNNNCC\n",
      "# Added wt_geo_paper_pam_NNNNCWAA\n",
      "# Added igeo_NNNNCNNN\n",
      "!!!!Skipping wt_nme2cas9_NNNNCC\n",
      "# Added inme_prelim1_NNNNCH\n",
      "#### PROCESSING: inme_prelim1_NNNNCH\n",
      "# Added wt_geo_paper_pam_NNNNCWAA\n",
      "# Added igeo_NNNNCNNN\n",
      "# Added wt_nme2cas9_NNNNCC\n",
      "!!!!Skipping inme_prelim1_NNNNCH\n",
      "           Coordinate_ID\n",
      "0     chr1 1_113757502_+\n",
      "1   chr11 11_131939841_+\n",
      "2    chr11 11_62626203_+\n",
      "3    chr17 17_66607536_+\n",
      "4    chr18 18_29779273_+\n",
      "5    chr19 19_55115698_+\n",
      "6      chr2 2_23448151_+\n",
      "7    chr22 22_48248410_+\n",
      "8      chr3 3_83803428_+\n",
      "9      chr4 4_65856415_+\n",
      "10     chr9 9_87344420_+\n",
      "11     chrX X_98889206_+\n"
     ]
    }
   ],
   "source": [
    "#### CONSOLIDATE CAS-OFFINDER OUTPUT FILES ####\n",
    "casoff_dir = \"/groups/doudna/projects/mtrinidad_projects/Geo_Cas9_Indel_HDR_KC/iGEO_Off_Target_Analysis/Off_Target_Prediction/CasOffinder_Results/\"\n",
    "casoff_cols = [\"Id\", \"Bulge type\", \"crRNA\", \"DNA\", \"Chromosome\", \"Location\", \"Direction\", \"Mismatches\", \"Bulge Size\"]\n",
    "ot_df_dict = {}\n",
    "coord_dict = {}\n",
    "seq_dict = {}\n",
    "\n",
    "for output_file in [x.replace(pwd, casoff_dir) for x in output_files]: # Note: Output files reorganized\n",
    "    base = output_file.split(\"/\")[-1].split(\"_Output_File_v3.txt\")[0]\n",
    "    pam = base.split(\"_\")[-1]\n",
    "    protein = base.split(f\"_{pam}\")[0]\n",
    "    print(f\"#### PROCESSING: {base}, {protein}, {pam}\")\n",
    "\n",
    "    # READ CASOFFINDER RESULTS\n",
    "    temp_df = pd.read_csv(output_file, sep=\"\\t\", names=casoff_cols, skiprows=2)\n",
    "    print(f\"# N Unfiltered: {temp_df.shape[0]}\")\n",
    "    \n",
    "    # FLAG OFF-TARGETS\n",
    "    temp_df[\"Validation_String\"] = temp_df.apply(generate_validation_str, trim_len = len(pam), axis = 1)\n",
    "    temp_df['Num_Seed_Diffs'] = temp_df[\"Validation_String\"].str[-10:-3].apply(count_mismatches)\n",
    "    temp_df['Pass_Seed_Filter'] = temp_df[\"Validation_String\"].str[-10:-3].apply(count_mismatches) <= 2  \n",
    "    temp_df[\"Proximal_2bp_Diffs\"] = temp_df[\"Validation_String\"].str[-2:].apply(count_mismatches)\n",
    "    temp_df[\"Distal_3bp_Diffs\"] = temp_df[\"Validation_String\"].str[0:3].apply(count_mismatches)\n",
    "    temp_df[\"Middle_Diffs\"] = temp_df[\"Validation_String\"].str[3:-10].apply(count_mismatches)\n",
    "    temp_df[\"CasOffinder_Output\"] = base\n",
    "\n",
    "    # FILTER AND SORT\n",
    "    filtered_df = temp_df.loc[temp_df.Pass_Seed_Filter == True]\n",
    "    filtered_df.sort_values([\"Middle_Diffs\"], ascending=True, inplace = True) # \"Mismatches\", \"Bulge Size\"\n",
    "    filtered_df[\"Coordinate_ID\"] = filtered_df['Chromosome'].astype(str) + '_' + filtered_df['Location'].astype(str) + '_' + \\\n",
    "                                    filtered_df['Direction'].astype(str)\n",
    "    coord_dict[base] = filtered_df.Coordinate_ID.unique()\n",
    "    ot_df_dict[base] = filtered_df\n",
    "    print(f\"# TOTAL OFF-TARGETS (Passing < 2 Seed-Region Mismatches & Bulges): {filtered_df.shape[0]}\")\n",
    "\n",
    "## LABEL COMMON OFF_TARGETS AMONGST EDITORS AND GUIDES\n",
    "grna_id_dict = {0:\"EMX1-AllT1\", \n",
    "                1:\"EMX1-AllT2\", \n",
    "                2:\"AAVS1-AllT1\", \n",
    "                3:\"AAVS1-AllT2\"}\n",
    "for ot_analysis_1, ot_df in ot_df_dict.items():\n",
    "    print(f\"#### PROCESSING: {ot_analysis_1}\")\n",
    "    for ot_analysis_2,coords in coord_dict.items():\n",
    "        if ot_analysis_1 == ot_analysis_2:\n",
    "            print(f\"!!!!Skipping {ot_analysis_2}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"# Added {ot_analysis_2}\")\n",
    "            new_col = f\"OT_In_{ot_analysis_2}\"\n",
    "            ot_df[new_col] = 0\n",
    "            ot_df.loc[ot_df.Coordinate_ID.isin(coords), new_col] = 1\n",
    "    ot_df = ot_df.loc[ot_df.Chromosome.str.contains(\"chr\")]\n",
    "    ot_df[\"Id\"] = ot_df[\"Id\"].apply(lambda x: grna_id_dict[x])\n",
    "    ot_df_dict[ot_analysis_1] = ot_df\n",
    "    ot_df.to_csv(f\"{ot_analysis_1}_OffTargets.csv\")\n",
    "\n",
    "## FIND COMMON OTs\n",
    "overlap_cols = [f\"OT_In_{x}\" for x in coord_dict.keys()]\n",
    "to_merge = [v for k,v in ot_df_dict.items()]\n",
    "all_dfs = pd.concat(to_merge, ignore_index=True)\n",
    "common_entries = all_dfs.groupby(['Coordinate_ID'])['CasOffinder_Output'].nunique()\n",
    "\n",
    "# Filter groups that appear in all DataFrames\n",
    "common_entries = common_entries[common_entries == len(to_merge)]\n",
    "\n",
    "# Reset index\n",
    "common_entries = common_entries.reset_index()[['Coordinate_ID']]\n",
    "print(common_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d3cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25275f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_Py_3_9",
   "language": "python",
   "name": "test_py_3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
